{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kooler/dev/sw/sns-fake-content/dataset/env_dataset/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "import shutil \n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gossipcop_fake = pd.read_json('../FakeNewsNet/data/gossipcop_fake.json')\n",
    "df_gossipcop_real = pd.read_json('../FakeNewsNet/data/gossipcop_real.json')\n",
    "df_politifact_fake = pd.read_json('../FakeNewsNet/data/politifact_fake.json')\n",
    "df_politifact_real = pd.read_json('../FakeNewsNet/data/politifact_real.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Master DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Dataset\n",
    "fake_df = pd.concat([df_gossipcop_fake, df_politifact_fake], ignore_index=True)\n",
    "real_df = pd.concat([df_gossipcop_real, df_politifact_real], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df['label'] = 'false'\n",
    "real_df['label'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.concat([fake_df, real_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.to_pickle('./master_fnn.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.read_pickle('./master_fnn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = master_df[master_df['lang'] == 'en']\n",
    "text_df = text_df[['text', 'label']]\n",
    "text_df['text'] = text_df['text'].astype(str)\n",
    "text_df['label'] = text_df['label'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.to_pickle('./text_fnn.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slice Dataset\n",
    "\n",
    "| Size | Amount |\n",
    "| ---- | ------ | \n",
    "| 3xs | 3000 |\n",
    "| 2xs | 10000 |\n",
    "| xs | 2000 |\n",
    "| s | 80000 |\n",
    "| m | 160000 |\n",
    "| l | ~500000 |\n",
    "| xl | all ~1300000 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3xs': 3000,\n",
       " '2xs': 10000,\n",
       " 'xs': 20000,\n",
       " 's': 80000,\n",
       " 'm': 160000,\n",
       " 'l': 500000,\n",
       " 'xl': 1368187}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = [3000, 10000, 20000, 80000, 160000, 500000, text_df.shape[0]]\n",
    "name = ['3xs', '2xs', 'xs', 's', 'm', 'l', 'xl']\n",
    "output_sizes = dict(zip(name, size))\n",
    "output_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(X, y):\n",
    "    return {\n",
    "        \"shape\": X.shape,\n",
    "        \"true\": np.count_nonzero(y == \"true\"),\n",
    "        \"false\": len(y) - np.count_nonzero(y == 'true'),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "932369"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_df[text_df[\"label\"] == 'true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, size):\n",
    "    true_size = min(len(df[df[\"label\"] == 'true']), size)\n",
    "    false_size = min(len(df[df[\"label\"] == 'false']), size)\n",
    "    \n",
    "    df_split = pd.concat(\n",
    "        [\n",
    "            df[df[\"label\"] == 'true'].sample(true_size),\n",
    "            df[df[\"label\"] == 'false'].sample(false_size),\n",
    "        ]\n",
    "    )\n",
    "    return df_split.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created /home/kooler/dev/sw/sns-fake-content/dataset/processed_data/transformer_datasets/fnn_3xs.zip\n",
      "{'train': {'shape': (3840,), 'true': 1897, 'false': 1943}, 'val': {'shape': (960,), 'true': 484, 'false': 476}, 'test': {'shape': (1200,), 'true': 619, 'false': 581}}\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created /home/kooler/dev/sw/sns-fake-content/dataset/processed_data/transformer_datasets/fnn_2xs.zip\n",
      "{'train': {'shape': (12800,), 'true': 6425, 'false': 6375}, 'val': {'shape': (3200,), 'true': 1576, 'false': 1624}, 'test': {'shape': (4000,), 'true': 1999, 'false': 2001}}\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created /home/kooler/dev/sw/sns-fake-content/dataset/processed_data/transformer_datasets/fnn_xs.zip\n",
      "{'train': {'shape': (25600,), 'true': 12769, 'false': 12831}, 'val': {'shape': (6400,), 'true': 3227, 'false': 3173}, 'test': {'shape': (8000,), 'true': 4004, 'false': 3996}}\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created /home/kooler/dev/sw/sns-fake-content/dataset/processed_data/transformer_datasets/fnn_s.zip\n",
      "{'train': {'shape': (102400,), 'true': 51105, 'false': 51295}, 'val': {'shape': (25600,), 'true': 12847, 'false': 12753}, 'test': {'shape': (32000,), 'true': 16048, 'false': 15952}}\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created /home/kooler/dev/sw/sns-fake-content/dataset/processed_data/transformer_datasets/fnn_m.zip\n",
      "{'train': {'shape': (204800,), 'true': 102510, 'false': 102290}, 'val': {'shape': (51200,), 'true': 25503, 'false': 25697}, 'test': {'shape': (64000,), 'true': 31987, 'false': 32013}}\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created /home/kooler/dev/sw/sns-fake-content/dataset/processed_data/transformer_datasets/fnn_l.zip\n",
      "{'train': {'shape': (598923,), 'true': 319753, 'false': 279170}, 'val': {'shape': (149731,), 'true': 80129, 'false': 69602}, 'test': {'shape': (187164,), 'true': 100118, 'false': 87046}}\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created /home/kooler/dev/sw/sns-fake-content/dataset/processed_data/transformer_datasets/fnn_xl.zip\n",
      "{'train': {'shape': (875639,), 'true': 596502, 'false': 279137}, 'val': {'shape': (218910,), 'true': 149458, 'false': 69452}, 'test': {'shape': (273638,), 'true': 186409, 'false': 87229}}\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "for name, size in output_sizes.items():\n",
    "    split_df = split_data(text_df, size)\n",
    "    \n",
    "    # Train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        split_df['text'], split_df[\"label\"], test_size=0.2, random_state=2023\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=2023\n",
    "    )\n",
    "    \n",
    "    data_ds = DatasetDict()\n",
    "\n",
    "    data_ds['train'] = Dataset.from_pandas(pd.concat([X_train, y_train], axis=1).astype(str)).class_encode_column(\"label\")\n",
    "    data_ds['validation'] = Dataset.from_pandas(pd.concat([X_val, y_val], axis=1).astype(str)).class_encode_column(\"label\")\n",
    "    data_ds['test'] = Dataset.from_pandas(pd.concat([X_test, y_test], axis=1).astype(str)).class_encode_column(\"label\")\n",
    "\n",
    "    data_ds = data_ds.remove_columns(['__index_level_0__'])\n",
    "    \n",
    "    path = f'./transformer_datasets/fnn_{name}'\n",
    "    data_ds.save_to_disk(path)\n",
    "    \n",
    "    zip_path = f'{path}.zip'\n",
    "    archived = shutil.make_archive(path, 'zip', path)\n",
    "    \n",
    "    print(\"File created\", archived)\n",
    "    print({\n",
    "        \"train\": get_shape(X_train, y_train),\n",
    "        \"val\": get_shape(X_val, y_val),\n",
    "        \"test\": get_shape(X_test, y_test),\n",
    "    })\n",
    "    print(\"=================================\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1368187/1368187 [08:47<00:00, 2594.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from preprocess import text_preprocess\n",
    "text_df['processed_text'] = text_preprocess(text_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created ./datasets/fnn_3xs.csv\n",
      "File created ./datasets/fnn_3xs.parquet.gzip\n",
      "File created ./datasets/fnn_3xs.pkl\n",
      "{'shape': (6000, 3)}\n",
      "=================================\n",
      "File created ./datasets/fnn_2xs.csv\n",
      "File created ./datasets/fnn_2xs.parquet.gzip\n",
      "File created ./datasets/fnn_2xs.pkl\n",
      "{'shape': (20000, 3)}\n",
      "=================================\n",
      "File created ./datasets/fnn_xs.csv\n",
      "File created ./datasets/fnn_xs.parquet.gzip\n",
      "File created ./datasets/fnn_xs.pkl\n",
      "{'shape': (40000, 3)}\n",
      "=================================\n",
      "File created ./datasets/fnn_s.csv\n",
      "File created ./datasets/fnn_s.parquet.gzip\n",
      "File created ./datasets/fnn_s.pkl\n",
      "{'shape': (160000, 3)}\n",
      "=================================\n",
      "File created ./datasets/fnn_m.csv\n",
      "File created ./datasets/fnn_m.parquet.gzip\n",
      "File created ./datasets/fnn_m.pkl\n",
      "{'shape': (320000, 3)}\n",
      "=================================\n",
      "File created ./datasets/fnn_l.csv\n",
      "File created ./datasets/fnn_l.parquet.gzip\n",
      "File created ./datasets/fnn_l.pkl\n",
      "{'shape': (935818, 3)}\n",
      "=================================\n",
      "File created ./datasets/fnn_xl.csv\n",
      "File created ./datasets/fnn_xl.parquet.gzip\n",
      "File created ./datasets/fnn_xl.pkl\n",
      "{'shape': (1368187, 3)}\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "# Store CSV\n",
    "for name, size in output_sizes.items():\n",
    "    split_df = split_data(text_df, size)\n",
    "    \n",
    "    split_df.to_csv(f'./datasets/fnn_{name}.csv', index=False)\n",
    "    print(\"File created\", f'./datasets/fnn_{name}.csv')\n",
    "    \n",
    "    split_df.to_parquet(f'./datasets/fnn_{name}.parquet.gzip', index=False)\n",
    "    print(\"File created\", f'./datasets/fnn_{name}.parquet.gzip')\n",
    "    \n",
    "    split_df.to_pickle(f'./datasets/fnn_{name}.pkl')\n",
    "    print(\"File created\", f'./datasets/fnn_{name}.pkl')\n",
    "    \n",
    "    print({'shape': split_df.shape})\n",
    "    print(\"=================================\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_database",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
